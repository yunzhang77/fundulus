The Juniata College HHMI cluster consists of a headnode and four compute nodes. Jobs are submi;ed to a handler which then delegates the cluster resources and distributes the job across the compute nodes. Jobs (programs) which are not submi;ed will run right on the headnode and will not take advantage of the processors on the compute nodes. Users are discouraged from running jobs on the headnode. The following is a short descripCon of procedures for submiDng jobs to the cluster.
The cluster has a batch server referred to as the cluster management server running on the headnode. This batch server monitors the status of the cluster and controls/monitors the various queues and job lists. Tied into the batch server, a scheduler makes decisions about how a job should be run and its placement in the queue. qsub interfaces into the batch server and lets it know that there is another job that has requested resources on the cluster. Once a job has been received by the batch server, the scheduler decides the placement and noCfies the batch server which in turn noCfies qsub (Torque/PBS) whether the job can be run or not. The current status (whether the job was successfully scheduled or not) is then returned to the user.
The script file is read by the qsub command. Qsub then acts upon any direcCves found in the script. When the job is created, a copy of the script file is made and that copy cannot be modified.
Below is a simple script for submiDng jobs to qsub. Run it as a test and then copy it to a new filename and edit it for the program you wish to run.
-Use nano to create and edit a .ile in your user’s directory called hostname.qsub -Copy these lines to hostname.qsub.
     #!/bin/bash
     #PBS -k o
     #PBS -l nodes=2:ppn=8,walltime=01:00
     #PBS -N hostname.pbs
     #PBS -j oe
#PBS -V
     module load openmpi-1.6.3
     module load open64
     mpirun hostname
-This will run the command hostname across 2 nodes on 8 cores each. -Submit this job by typing
     $ qsub hostname.qsub
-This should only take a couple seconds to run. There should then be a .ile in your user’s home directory called hostname.pbs.oXXX. This should contain the 8 hostnames of 2 different nodes.
To modify this to run your own jobs:
- set the nodes= to the number of nodes you want to run on. HHMI has 4 nodes so this should be a number
between 1 and 4.
-Set the ppn= to the number of cores (processors) per node you want to use. The maximum number of
processors that may be used is 32. Thus to use 4 nodes, the ppn should be set to 8 if you want to use a
total of 32.
-Change –N hostname.pbs to –N your job name in the queue.
-oe This directive tells PBS to put both normal output and error output into the same output .ile.
–V speci.ies that your current environment variables should be exported to the job. For example, if the path
to your executable is found in your PATH variable and your script contains the line “#PBS –V”, then the path will also be known to the batch job.
-leave the module lines as is but any other modules you wish to load can be added with the same syntax. -leave mpirun but change hostname to the command (/path_to_executable/program_name arg1 arg2 ...) you want to run after mpirun.
Other useful related commands include:
$ qstat – displays jobs running in the queue along with the job ID.
$checkjob job_id - will give detailed information about your job. $ qnodes – displays current state of the compute nodes.
For more informaCon on qsub, type man qsub at the command line ($).
